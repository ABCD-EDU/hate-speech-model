{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arian\\.conda\\envs\\ai-ds-test4\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchmetrics.functional import accuracy, f1_score, auroc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text    0\n",
       "HOF     0\n",
       "NOT     0\n",
       "NONE    0\n",
       "OFFN    0\n",
       "PRFN    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_df = pd.read_csv('../res/preprocessed/task1_2/task1_2.csv')\n",
    "task_df = task_df.dropna()\n",
    "task_df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2594\n",
      "556\n",
      "556\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_df, val_df = train_test_split(task_df, test_size=0.3)\n",
    "# Divide validation df to validation and test dataframes\n",
    "val_df, test_df = train_test_split(val_df, test_size=0.5)\n",
    "print(len(train_df))\n",
    "print(len(val_df))\n",
    "print(len(test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'HOF', 1: 'NOT'}\n",
      "{'HOF': 0, 'NOT': 1}\n",
      "{0: 'NONE', 1: 'OFFN', 2: 'PRFN'}\n",
      "{'NONE': 0, 'OFFN': 1, 'PRFN': 2}\n"
     ]
    }
   ],
   "source": [
    "LABEL_COLUMNS = list(train_df.columns)\n",
    "LABEL_COLUMNS.remove('text')\n",
    "\n",
    "TASK1_LABELS = LABEL_COLUMNS[:2]\n",
    "TASK2_LABELS = LABEL_COLUMNS[2:]\n",
    "\n",
    "\n",
    "task1_id2label = {idx: label for idx, label in enumerate(TASK1_LABELS)}\n",
    "task1_label2id = {label: idx for idx, label in enumerate(TASK1_LABELS)}\n",
    "\n",
    "task2_label2id = {label: idx for idx, label in enumerate(TASK2_LABELS)}\n",
    "task2_id2label = {idx: label for idx, label in enumerate(TASK2_LABELS)}\n",
    "\n",
    "print(task1_id2label)\n",
    "print(task1_label2id)\n",
    "print(task2_id2label)\n",
    "print(task2_label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, data_df, tokenizer, max_token_len: 256, batch_size=16):\n",
    "        self.data = data_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_row = self.data.iloc[idx]\n",
    "        text = data_row.text\n",
    "        labels1 = data_row[TASK1_LABELS]\n",
    "        labels2 = data_row[TASK2_LABELS]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_token_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return dict(\n",
    "            input_ids=encoding['input_ids'].flatten(),\n",
    "            attention_mask=encoding['attention_mask'].flatten(),\n",
    "            labels1=torch.FloatTensor(labels1),\n",
    "            labels2=torch.FloatTensor(labels2)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "train_dataset = TwitterDataset(train_df, tokenizer, max_token_len=256)\n",
    "test_dataset = TwitterDataset(test_df, tokenizer, max_token_len=256)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045, 24637,  ...,     0,     0,     0],\n",
       "         [  101,  2017,  2024,  ...,     0,     0,     0],\n",
       "         [  101,  1045,  2215,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101, 16245,  4679,  ...,     0,     0,     0],\n",
       "         [  101,  2031,  2017,  ...,     0,     0,     0],\n",
       "         [  101,  2065,  2017,  ...,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels1': tensor([[0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.]]),\n",
       " 'labels2': tensor([[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.],\n",
       "         [1., 0., 0.],\n",
       "         [1., 0., 0.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 1.]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = iter(test_loader).next()\n",
    "sample_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 256]), torch.Size([8, 256]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch[\"input_ids\"].shape, sample_batch[\"attention_mask\"].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TwitterDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_df, val_df, test_df, tokenizer, batch_size=12, max_token_len=256):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = TwitterDataset(\n",
    "            self.train_df,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "        )\n",
    "        self.val_dataset = TwitterDataset(\n",
    "            self.val_df,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "        )\n",
    "        self.test_dataset = TwitterDataset(\n",
    "            self.test_df,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=num_workers,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=num_workers,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 2\n",
    "BATCH_SIZE = 12\n",
    "MAX_TOKEN_COUNT = 256\n",
    "\n",
    "data_module = TwitterDataModule(\n",
    "    train_df,\n",
    "    val_df,\n",
    "    test_df,\n",
    "    tokenizer,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    max_token_len=config[\"max_token_count\"]\n",
    ")\n",
    "data_module.setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterNeuralNet(pl.LightningModule):\n",
    "    def __init__(self, task1_n_classes: int, task2_n_classes: int,n_training_steps=None, n_warmup_steps=None):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\n",
    "            BERT_MODEL_NAME, return_dict=True)\n",
    "        self.hidden = nn.Linear(self.bert.config.hidden_size,\n",
    "                                self.bert.config.hidden_size)\n",
    "                    \n",
    "        self.task1_classifier = nn.Linear(self.bert.config.hidden_size, task1_n_classes)\n",
    "\n",
    "        self.task2_classifier = nn.Linear(self.bert.config.hidden_size, task2_n_classes)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.hidden.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.task1_classifier.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.task2_classifier.weight)\n",
    "\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        # print(input_ids.shape)\n",
    "        # print(attention_mask.shape)\n",
    "        pooled_output = torch.mean(output.last_hidden_state, 1)\n",
    "        pooled_output = self.hidden(pooled_output)\n",
    "        # pooled_output = self.dropout(pooled_output)\n",
    "        pooled_output = F.relu(pooled_output)\n",
    "        \n",
    "        output1 = self.task1_classifier(pooled_output)\n",
    "        output2 = self.task2_classifier(pooled_output)\n",
    "         \n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss1 = self.criterion(output1, labels['labels1'])\n",
    "            loss2 = self.criterion(output2, labels['labels2'])\n",
    "            loss = loss1+loss2\n",
    "            \n",
    "        return loss, [output1, output2]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = {}\n",
    "        labels['labels1'] = batch[\"labels1\"]\n",
    "        labels['labels2'] = batch[\"labels2\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = {}\n",
    "        labels['labels1'] = batch[\"labels1\"]\n",
    "        labels['labels2'] = batch[\"labels2\"]\n",
    "        loss,outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return {\"val_loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return outputs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=2e-5)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.n_warmup_steps,\n",
    "            num_training_steps=self.n_training_steps\n",
    "        )\n",
    "        return dict(\n",
    "            optimizer=optimizer,\n",
    "            lr_scheduler=dict(\n",
    "                scheduler=scheduler,\n",
    "                interval='step'\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 432)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch = len(train_df) // BATCH_SIZE\n",
    "total_training_steps = steps_per_epoch * N_EPOCHS\n",
    "steps_per_epoch, total_training_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86, 432)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warmup_steps = total_training_steps // 5\n",
    "warmup_steps, total_training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = TwitterNeuralNet(\n",
    "    task1_n_classes=len(TASK1_LABELS),\n",
    "    task2_n_classes=len(TASK2_LABELS),\n",
    "    n_warmup_steps=warmup_steps,\n",
    "    n_training_steps=total_training_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.2434,  0.1023],\n",
       "         [-0.0088,  0.0569],\n",
       "         [ 0.1594, -0.1648],\n",
       "         [ 0.0766, -0.0815],\n",
       "         [-0.1504, -0.0257],\n",
       "         [ 0.0506, -0.0468],\n",
       "         [-0.1782,  0.1036],\n",
       "         [-0.1891, -0.0928]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[-0.3534, -0.2036,  0.4339],\n",
       "         [-0.2159, -0.1994,  0.4214],\n",
       "         [-0.4694, -0.2018,  0.1452],\n",
       "         [-0.2388, -0.2643,  0.2971],\n",
       "         [-0.1931, -0.1378,  0.6452],\n",
       "         [-0.2965, -0.3154,  0.4177],\n",
       "         [-0.2580, -0.2072,  0.2522],\n",
       "         [-0.3008, -0.3383,  0.2573]], grad_fn=<AddmmBackward0>)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, predictions = model(\n",
    "    sample_batch[\"input_ids\"], sample_batch[\"attention_mask\"])\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(\"./lightning_logs\",\n",
    "                           'twitter-task-all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arian\\.conda\\envs\\ai-ds-test4\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:151: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x0000011CFD6ADE80>)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x0000011CFD6ADE80>)`.\n",
      "  rank_zero_deprecation(\n",
      "C:\\Users\\Arian\\.conda\\envs\\ai-ds-test4\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=3)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Arian\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name             | Type              | Params\n",
      "-------------------------------------------------------\n",
      "0 | bert             | BertModel         | 109 M \n",
      "1 | hidden           | Linear            | 590 K \n",
      "2 | task1_classifier | Linear            | 1.5 K \n",
      "3 | task2_classifier | Linear            | 2.3 K \n",
      "4 | criterion        | BCEWithLogitsLoss | 0     \n",
      "5 | dropout          | Dropout           | 0     \n",
      "-------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.307   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arian\\.conda\\envs\\ai-ds-test4\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arian\\.conda\\envs\\ai-ds-test4\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 264/264 [05:52<00:00,  1.34s/it, loss=0.394, v_num=11, train_loss=0.279, val_loss=0.487] \n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "   max_epochs=N_EPOCHS,\n",
    "   gpus=1,\n",
    "   progress_bar_refresh_rate=3,\n",
    "   logger=logger,\n",
    "   checkpoint_callback=checkpoint_callback, num_sanity_val_steps=10,\n",
    "   fast_dev_run=False,\n",
    "   )\n",
    "\n",
    "\n",
    "\n",
    "trainer.fit(model, data_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "PATH = './saved_model'\n",
    "torch.save(model.state_dict(), os.path.join(PATH, \"model.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "trained_model = copy.deepcopy(model)\n",
    "trained_model.eval()\n",
    "trained_model.freeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK 1\n",
      "tensor([[ 4.2146, -3.7787]])\n",
      "{0: 'HOF', 1: 'NOT'}\n",
      "HOF\n",
      "TASK 2\n",
      "tensor([[-4.3500, -1.2555,  1.3852]])\n",
      "{0: 'NONE', 1: 'OFFN', 2: 'PRFN'}\n",
      "PRFN\n"
     ]
    }
   ],
   "source": [
    "test_text = 'Hey asshole, go kill yourself'\n",
    "test_text = tokenizer.encode_plus(\n",
    "    test_text,\n",
    "    add_special_tokens=True,\n",
    "    max_length=256,\n",
    "    return_token_type_ids=False,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "with torch.no_grad():\n",
    "    test_input_ids, test_att_mask = test_text['input_ids'], test_text['attention_mask']\n",
    "    _, output = trained_model(test_input_ids, test_att_mask)\n",
    "\n",
    "# _, output = model(test_input_ids.unsqueeze(0), test_att_mask.unsqueeze(0),None)\n",
    "print('TASK 1')\n",
    "print(output[0])\n",
    "print(task1_id2label)\n",
    "print(task1_id2label[int(torch.argmax(output[0]))])\n",
    "\n",
    "print('TASK 2')\n",
    "print(output[1])\n",
    "print(task2_id2label)\n",
    "print(task2_id2label[int(torch.argmax(output[1]))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i texted my boss that i am sick and he never replied lol\n",
      "TASK 1               True: NOT |  Prediction: NOT\n",
      "tensor([0., 1.])\n",
      "tensor([[-2.6155,  2.3481]], device='cuda:0')\n",
      "TASK 2               True: NONE |  Prediction: NONE\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[ 1.5972, -3.2779, -2.4990]], device='cuda:0')\n",
      "--------------------\n",
      "there is not much in the barrel to scrape personally i thought maybot would have chosen chris grayling\n",
      "TASK 1               True: NOT |  Prediction: NOT\n",
      "tensor([0., 1.])\n",
      "tensor([[-3.2557,  3.2370]], device='cuda:0')\n",
      "TASK 2               True: NONE |  Prediction: NONE\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[ 3.2259, -3.6569, -4.0340]], device='cuda:0')\n",
      "--------------------\n",
      "young kids abuse pm modi in priyanka vadras presence\n",
      "TASK 1               True: NOT |  Prediction: NOT\n",
      "tensor([0., 1.])\n",
      "tensor([[-1.8651,  1.8223]], device='cuda:0')\n",
      "TASK 2               True: NONE |  Prediction: NONE\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[ 1.4282, -2.7726, -4.2780]], device='cuda:0')\n",
      "--------------------\n",
      "you know where i am at\n",
      "TASK 1               True: NOT |  Prediction: NOT\n",
      "tensor([0., 1.])\n",
      "tensor([[-4.4084,  4.6073]], device='cuda:0')\n",
      "TASK 2               True: NONE |  Prediction: NONE\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[ 3.9022, -4.7401, -4.1760]], device='cuda:0')\n",
      "--------------------\n",
      "vanessa rocked my pitch to the edge of the infieldoutfield on a fly in her game tonight then found out after the g\n",
      "TASK 1               True: NOT |  Prediction: NOT\n",
      "tensor([0., 1.])\n",
      "tensor([[-4.1416,  4.1414]], device='cuda:0')\n",
      "TASK 2               True: NONE |  Prediction: NONE\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[ 3.9593, -5.2578, -4.6801]], device='cuda:0')\n",
      "--------------------\n",
      " this is fucking insane imagine the depravity it takes to prevent kids from graduating and receiving their high school d\n",
      "TASK 1               True: HOF |  Prediction: HOF\n",
      "tensor([1., 0.])\n",
      "tensor([[ 4.2564, -4.1830]], device='cuda:0')\n",
      "TASK 2               True: PRFN |  Prediction: PRFN\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[-3.9192, -2.4582,  1.3809]], device='cuda:0')\n",
      "--------------------\n",
      "you are the only constant in my life you know that do not you\n",
      "TASK 1               True: NOT |  Prediction: NOT\n",
      "tensor([0., 1.])\n",
      "tensor([[-3.8159,  4.0002]], device='cuda:0')\n",
      "TASK 2               True: NONE |  Prediction: NONE\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[ 3.5884, -4.6348, -4.3805]], device='cuda:0')\n",
      "--------------------\n",
      "can never be me but you\n",
      "TASK 1               True: NOT |  Prediction: NOT\n",
      "tensor([0., 1.])\n",
      "tensor([[-3.3029,  3.7444]], device='cuda:0')\n",
      "TASK 2               True: NONE |  Prediction: NONE\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[ 3.8584, -4.2369, -4.7201]], device='cuda:0')\n",
      "--------------------\n",
      " there will never be another  all you young spittaz tryna copy his style are fake fucking frauds shit versions\n",
      "TASK 1               True: HOF |  Prediction: HOF\n",
      "tensor([1., 0.])\n",
      "tensor([[ 4.6659, -4.5726]], device='cuda:0')\n",
      "TASK 2               True: OFFN |  Prediction: PRFN\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[-4.4364, -0.8369,  0.4938]], device='cuda:0')\n",
      "--------------------\n",
      " i rather die with memories than with dreams\n",
      "TASK 1               True: NOT |  Prediction: NOT\n",
      "tensor([0., 1.])\n",
      "tensor([[-3.5525,  3.5129]], device='cuda:0')\n",
      "TASK 2               True: NONE |  Prediction: NONE\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[ 3.5226, -4.1361, -4.2722]], device='cuda:0')\n",
      "--------------------\n",
      " this shit bout fake ass hell\n",
      "TASK 1               True: HOF |  Prediction: HOF\n",
      "tensor([1., 0.])\n",
      "tensor([[ 4.7769, -4.7014]], device='cuda:0')\n",
      "TASK 2               True: PRFN |  Prediction: PRFN\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[-4.2838, -2.5655,  2.1357]], device='cuda:0')\n",
      "--------------------\n",
      "you are the man\n",
      "TASK 1               True: NOT |  Prediction: NOT\n",
      "tensor([0., 1.])\n",
      "tensor([[-2.5529,  2.6336]], device='cuda:0')\n",
      "TASK 2               True: NONE |  Prediction: NONE\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[ 2.7987, -3.2919, -4.2400]], device='cuda:0')\n",
      "--------------------\n",
      "its not appropriate to call rudy giuliani a traitor because that would assume hes coherent when he speaks\n",
      "TASK 1               True: NOT |  Prediction: NOT\n",
      "tensor([0., 1.])\n",
      "tensor([[-1.7429e-04,  1.8758e-01]], device='cuda:0')\n",
      "TASK 2               True: NONE |  Prediction: NONE\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[ 0.0118, -0.6540, -3.5925]], device='cuda:0')\n",
      "--------------------\n",
      " moral of the story you all will spend that 25 when niggas is poppin but will not spend that 25 when niggas coming up yal\n",
      "TASK 1               True: NOT |  Prediction: NOT\n",
      "tensor([0., 1.])\n",
      "tensor([[-2.1218,  2.0478]], device='cuda:0')\n",
      "TASK 2               True: NONE |  Prediction: NONE\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[ 2.1073, -3.7330, -3.9361]], device='cuda:0')\n",
      "--------------------\n",
      "fuck jay rt\n",
      "TASK 1               True: HOF |  Prediction: HOF\n",
      "tensor([1., 0.])\n",
      "tensor([[ 5.1330, -5.2231]], device='cuda:0')\n",
      "TASK 2               True: PRFN |  Prediction: PRFN\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[-5.0722, -2.8199,  2.5223]], device='cuda:0')\n",
      "--------------------\n",
      "going to the grocery store and only buying toilet paper makes me worry people will look at me and think something i\n",
      "TASK 1               True: NOT |  Prediction: NOT\n",
      "tensor([0., 1.])\n",
      "tensor([[-3.4262,  3.4472]], device='cuda:0')\n",
      "TASK 2               True: NONE |  Prediction: NONE\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[ 3.1905, -4.0420, -4.0854]], device='cuda:0')\n",
      "--------------------\n",
      " when lauv said i wonder what it feels like to be more than i amdamn that hits me hard\n",
      "TASK 1               True: HOF |  Prediction: NOT\n",
      "tensor([1., 0.])\n",
      "tensor([[-3.1066,  3.0424]], device='cuda:0')\n",
      "TASK 2               True: PRFN |  Prediction: NONE\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[ 2.9329, -4.4197, -4.2213]], device='cuda:0')\n",
      "--------------------\n",
      "had to slow that shit down\n",
      "TASK 1               True: HOF |  Prediction: HOF\n",
      "tensor([1., 0.])\n",
      "tensor([[ 3.7989, -3.8564]], device='cuda:0')\n",
      "TASK 2               True: PRFN |  Prediction: PRFN\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[-3.7725, -3.7749,  3.0095]], device='cuda:0')\n",
      "--------------------\n",
      " every time i go on a rant i lose followers but honestly if you are anti vax or anti choice i do not want you here anyway\n",
      "TASK 1               True: NOT |  Prediction: NOT\n",
      "tensor([0., 1.])\n",
      "tensor([[-2.0577,  2.3258]], device='cuda:0')\n",
      "TASK 2               True: NONE |  Prediction: NONE\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[ 1.8972, -2.7709, -3.4807]], device='cuda:0')\n",
      "--------------------\n",
      "arggg i get was thinking that seems like a very stressful way of doing something lovely work as usual h\n",
      "TASK 1               True: NOT |  Prediction: NOT\n",
      "tensor([0., 1.])\n",
      "tensor([[-3.8009,  3.8717]], device='cuda:0')\n",
      "TASK 2               True: NONE |  Prediction: NONE\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[ 3.8837, -4.6557, -3.9930]], device='cuda:0')\n",
      "--------------------\n",
      " we get to the bag regardless of the journey regardless of the time spent getting there we get there eventually\n",
      "TASK 1               True: NOT |  Prediction: NOT\n",
      "tensor([0., 1.])\n",
      "tensor([[-4.2206,  4.2581]], device='cuda:0')\n",
      "TASK 2               True: NONE |  Prediction: NONE\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[ 4.0948, -5.0449, -4.5374]], device='cuda:0')\n",
      "--------------------\n",
      " spoilerswill that be all mr starkthat will be all ms pottshey pepsit is okay tony you can rest now\n",
      "TASK 1               True: NOT |  Prediction: NOT\n",
      "tensor([0., 1.])\n",
      "tensor([[-3.5556,  3.5507]], device='cuda:0')\n",
      "TASK 2               True: NONE |  Prediction: NONE\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[ 3.6724, -3.8050, -3.5354]], device='cuda:0')\n",
      "--------------------\n",
      "i can get behind this\n",
      "TASK 1               True: NOT |  Prediction: NOT\n",
      "tensor([0., 1.])\n",
      "tensor([[-4.1770,  4.0753]], device='cuda:0')\n",
      "TASK 2               True: NONE |  Prediction: NONE\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[ 4.0012, -4.8545, -4.3370]], device='cuda:0')\n",
      "--------------------\n",
      " did this bitch just\n",
      "TASK 1               True: HOF |  Prediction: HOF\n",
      "tensor([1., 0.])\n",
      "tensor([[ 4.0940, -3.8989]], device='cuda:0')\n",
      "TASK 2               True: OFFN |  Prediction: PRFN\n",
      "tensor([0., 1., 0.])\n",
      "tensor([[-3.8424, -2.0252,  1.3570]], device='cuda:0')\n",
      "--------------------\n",
      " tonight tonight tonight catch me doing some standup at the floetic fusion showcase in nw dc ht\n",
      "TASK 1               True: NOT |  Prediction: NOT\n",
      "tensor([0., 1.])\n",
      "tensor([[-3.3807,  3.3092]], device='cuda:0')\n",
      "TASK 2               True: NONE |  Prediction: NONE\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[ 3.7916, -5.0519, -4.5143]], device='cuda:0')\n",
      "--------------------\n",
      "steph curry is my fatherpavy\n",
      "TASK 1               True: NOT |  Prediction: NOT\n",
      "tensor([0., 1.])\n",
      "tensor([[-2.7772,  2.4679]], device='cuda:0')\n",
      "TASK 2               True: NONE |  Prediction: NONE\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[ 2.3461, -3.7907, -3.5857]], device='cuda:0')\n",
      "--------------------\n",
      " there have been cases of sex crimes of jihad against hindus in the year alone this is not an estimate these\n",
      "TASK 1               True: NOT |  Prediction: NOT\n",
      "tensor([0., 1.])\n",
      "tensor([[-1.2283,  1.6752]], device='cuda:0')\n",
      "TASK 2               True: NONE |  Prediction: NONE\n",
      "tensor([1., 0., 0.])\n",
      "tensor([[ 1.4973, -2.9557, -3.8259]], device='cuda:0')\n",
      "--------------------\n",
      "i am tryna get someone is hand print smacked into my ass ja feel\n",
      "TASK 1               True: HOF |  Prediction: HOF\n",
      "tensor([1., 0.])\n",
      "tensor([[ 3.9302, -3.8057]], device='cuda:0')\n",
      "TASK 2               True: PRFN |  Prediction: PRFN\n",
      "tensor([0., 0., 1.])\n",
      "tensor([[-4.3169, -1.5746,  1.3400]], device='cuda:0')\n",
      "--------------------\n",
      "Accuracy Task1: 0.8974820143884892\n",
      "Accuracy Task2: 0.8345323741007195\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "trained_model = trained_model.to(device)\n",
    "\n",
    "task1_predictions = []\n",
    "task1_labels = []\n",
    "\n",
    "task2_predictions = []\n",
    "task2_labels = []\n",
    "\n",
    "counter = 0\n",
    "task1_true_counter = 0\n",
    "task2_true_counter = 0\n",
    "with torch.no_grad():\n",
    "    for idx, item in enumerate(test_dataset):\n",
    "        _, prediction = trained_model(\n",
    "            item['input_ids'].unsqueeze(dim=0).to(device),\n",
    "            item['attention_mask'].unsqueeze(dim=0).to(device)\n",
    "        )\n",
    "    \n",
    "        task1_predictions.append(prediction[0].flatten())\n",
    "        task2_predictions.append(prediction[1].flatten())\n",
    "\n",
    "        task1_true_label = task1_id2label[int(torch.argmax(item['labels1']))]\n",
    "        task2_true_label = task2_id2label[int(torch.argmax(item['labels2']))]\n",
    "\n",
    "\n",
    "\n",
    "        task1_guess_label = task1_id2label[int(torch.argmax(task1_predictions[counter]))]\n",
    "        task2_guess_label = task2_id2label[int(\n",
    "            torch.argmax(task2_predictions[counter]))]\n",
    "        \n",
    "\n",
    "        if counter % 20 == 0:\n",
    "          text_val = test_df.iloc[idx]['text']\n",
    "          print(text_val)\n",
    "          print(f'TASK 1 \\\n",
    "              True: {task1_true_label} |  Prediction: {task1_guess_label}')\n",
    "          print(item['labels1'])\n",
    "          print(prediction[0])\n",
    "          print(f'TASK 2 \\\n",
    "              True: {task2_true_label} |  Prediction: {task2_guess_label}')\n",
    "          print(item['labels2'])\n",
    "          print(prediction[1])\n",
    "          print('--------------------')\n",
    "        \n",
    "        if task1_true_label == task1_guess_label:\n",
    "            task1_true_counter += 1\n",
    "        if task2_true_label == task2_guess_label:\n",
    "            task2_true_counter += 1\n",
    "        \n",
    "        task1_labels.append(item['labels1'].int())\n",
    "        task2_labels.append(item['labels2'].int())\n",
    "        counter += 1\n",
    "\n",
    "print(f'Accuracy Task1: {task1_true_counter/len(test_dataset)}')\n",
    "print(f'Accuracy Task2: {task2_true_counter/len(test_dataset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_predictions_ = torch.stack(task1_predictions).detach().cpu()\n",
    "task1_labels_ = torch.stack(task1_labels).detach().cpu()\n",
    "\n",
    "task2_predictions_ = torch.stack(task2_predictions).detach().cpu()\n",
    "task2_labels_ = torch.stack(task2_labels).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task1_accuracy_score: 0.8965827226638794\n",
      "task2_accuracy_score: 0.8986810445785522\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD = 0.5\n",
    "\n",
    "task1_accuracy_score = accuracy(\n",
    "    task1_predictions_, task1_labels_, threshold=THRESHOLD)\n",
    "\n",
    "task2_accuracy_score = accuracy(\n",
    "    task2_predictions_, task2_labels_, threshold=THRESHOLD)\n",
    "\n",
    "print(f'task1_accuracy_score: {task1_accuracy_score}')\n",
    "print(f'task2_accuracy_score: {task2_accuracy_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (729958493.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [26]\u001b[1;36m\u001b[0m\n\u001b[1;33m    ksdk'askl;df\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "ksdk'askl;df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arian\\.conda\\envs\\ai-ds-test4\\lib\\site-packages\\torch\\_jit_internal.py:668: LightningDeprecationWarning: The `LightningModule.model_size` property was deprecated in v1.5 and will be removed in v1.7. Please use the `pytorch_lightning.utilities.memory.get_model_size_mb`.\n",
      "  if hasattr(mod, name):\n",
      "C:\\Users\\Arian\\.conda\\envs\\ai-ds-test4\\lib\\site-packages\\torch\\_jit_internal.py:669: LightningDeprecationWarning: The `LightningModule.model_size` property was deprecated in v1.5 and will be removed in v1.7. Please use the `pytorch_lightning.utilities.memory.get_model_size_mb`.\n",
      "  item = getattr(mod, name)\n",
      "C:\\Users\\Arian\\.conda\\envs\\ai-ds-test4\\lib\\site-packages\\torch\\_jit_internal.py:668: LightningDeprecationWarning: `LightningModule.use_amp` was deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.amp_backend`.\n",
      "  if hasattr(mod, name):\n",
      "C:\\Users\\Arian\\.conda\\envs\\ai-ds-test4\\lib\\site-packages\\torch\\_jit_internal.py:669: LightningDeprecationWarning: `LightningModule.use_amp` was deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.amp_backend`.\n",
      "  item = getattr(mod, name)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\BSCS 3- 2ND SEM\\9334-CS321-ARTIFICIAL_INTELLIGENCE\\GithubFinalProj\\hate-speech-model\\ai_model\\model_all\\bert_all_lightning.ipynb Cell 25'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/BSCS%203-%202ND%20SEM/9334-CS321-ARTIFICIAL_INTELLIGENCE/GithubFinalProj/hate-speech-model/ai_model/model_all/bert_all_lightning.ipynb#ch0000022?line=6'>7</a>\u001b[0m sample_att_mask \u001b[39m=\u001b[39m train_dataset[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/BSCS%203-%202ND%20SEM/9334-CS321-ARTIFICIAL_INTELLIGENCE/GithubFinalProj/hate-speech-model/ai_model/model_all/bert_all_lightning.ipynb#ch0000022?line=7'>8</a>\u001b[0m \u001b[39m# print(sample_att_mask)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/BSCS%203-%202ND%20SEM/9334-CS321-ARTIFICIAL_INTELLIGENCE/GithubFinalProj/hate-speech-model/ai_model/model_all/bert_all_lightning.ipynb#ch0000022?line=9'>10</a>\u001b[0m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39msave(torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mtrace(model,(sample_input_ids, sample_att_mask) ), os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(PATH, \u001b[39m\"\u001b[39m\u001b[39mmodel.pth\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\.conda\\envs\\ai-ds-test4\\lib\\site-packages\\torch\\jit\\_trace.py:741\u001b[0m, in \u001b[0;36mtrace\u001b[1;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=737'>738</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=739'>740</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(func, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m--> <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=740'>741</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m trace_module(\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=741'>742</a>\u001b[0m         func,\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=742'>743</a>\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mforward\u001b[39;49m\u001b[39m\"\u001b[39;49m: example_inputs},\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=743'>744</a>\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=744'>745</a>\u001b[0m         check_trace,\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=745'>746</a>\u001b[0m         wrap_check_inputs(check_inputs),\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=746'>747</a>\u001b[0m         check_tolerance,\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=747'>748</a>\u001b[0m         strict,\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=748'>749</a>\u001b[0m         _force_outplace,\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=749'>750</a>\u001b[0m         _module_class,\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=750'>751</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=752'>753</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=753'>754</a>\u001b[0m     \u001b[39mhasattr\u001b[39m(func, \u001b[39m\"\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=754'>755</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(func\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule)\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=755'>756</a>\u001b[0m     \u001b[39mand\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=756'>757</a>\u001b[0m ):\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=757'>758</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m trace_module(\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=758'>759</a>\u001b[0m         func\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m,\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=759'>760</a>\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m: example_inputs},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=766'>767</a>\u001b[0m         _module_class,\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=767'>768</a>\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\ai-ds-test4\\lib\\site-packages\\torch\\jit\\_trace.py:958\u001b[0m, in \u001b[0;36mtrace_module\u001b[1;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=953'>954</a>\u001b[0m     argument_names \u001b[39m=\u001b[39m get_callable_argument_names(func)\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=955'>956</a>\u001b[0m example_inputs \u001b[39m=\u001b[39m make_tuple(example_inputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=957'>958</a>\u001b[0m module\u001b[39m.\u001b[39;49m_c\u001b[39m.\u001b[39;49m_create_method_from_trace(\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=958'>959</a>\u001b[0m     method_name,\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=959'>960</a>\u001b[0m     func,\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=960'>961</a>\u001b[0m     example_inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=961'>962</a>\u001b[0m     var_lookup_fn,\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=962'>963</a>\u001b[0m     strict,\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=963'>964</a>\u001b[0m     _force_outplace,\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=964'>965</a>\u001b[0m     argument_names,\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=965'>966</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=966'>967</a>\u001b[0m check_trace_method \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_c\u001b[39m.\u001b[39m_get_method(method_name)\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/jit/_trace.py?line=968'>969</a>\u001b[0m \u001b[39m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\ai-ds-test4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\.conda\\envs\\ai-ds-test4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1098\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/nn/modules/module.py?line=1095'>1096</a>\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/nn/modules/module.py?line=1096'>1097</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "\u001b[1;32me:\\BSCS 3- 2ND SEM\\9334-CS321-ARTIFICIAL_INTELLIGENCE\\GithubFinalProj\\hate-speech-model\\ai_model\\model_all\\bert_all_lightning.ipynb Cell 11'\u001b[0m in \u001b[0;36mTwitterNeuralNet.forward\u001b[1;34m(self, input_ids, attention_mask, labels)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BSCS%203-%202ND%20SEM/9334-CS321-ARTIFICIAL_INTELLIGENCE/GithubFinalProj/hate-speech-model/ai_model/model_all/bert_all_lightning.ipynb#ch0000010?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids, attention_mask, labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/BSCS%203-%202ND%20SEM/9334-CS321-ARTIFICIAL_INTELLIGENCE/GithubFinalProj/hate-speech-model/ai_model/model_all/bert_all_lightning.ipynb#ch0000010?line=22'>23</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(input_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BSCS%203-%202ND%20SEM/9334-CS321-ARTIFICIAL_INTELLIGENCE/GithubFinalProj/hate-speech-model/ai_model/model_all/bert_all_lightning.ipynb#ch0000010?line=23'>24</a>\u001b[0m     pooled_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(output\u001b[39m.\u001b[39mlast_hidden_state, \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BSCS%203-%202ND%20SEM/9334-CS321-ARTIFICIAL_INTELLIGENCE/GithubFinalProj/hate-speech-model/ai_model/model_all/bert_all_lightning.ipynb#ch0000010?line=24'>25</a>\u001b[0m     pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden(pooled_output)\n",
      "File \u001b[1;32m~\\.conda\\envs\\ai-ds-test4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\.conda\\envs\\ai-ds-test4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1098\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/nn/modules/module.py?line=1095'>1096</a>\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/nn/modules/module.py?line=1096'>1097</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\bert\\modeling_bert.py:950\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/AppData/Roaming/Python/Python39/site-packages/transformers/models/bert/modeling_bert.py?line=946'>947</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/AppData/Roaming/Python/Python39/site-packages/transformers/models/bert/modeling_bert.py?line=947'>948</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Arian/AppData/Roaming/Python/Python39/site-packages/transformers/models/bert/modeling_bert.py?line=949'>950</a>\u001b[0m batch_size, seq_length \u001b[39m=\u001b[39m input_shape\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/AppData/Roaming/Python/Python39/site-packages/transformers/models/bert/modeling_bert.py?line=950'>951</a>\u001b[0m device \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mdevice \u001b[39mif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m inputs_embeds\u001b[39m.\u001b[39mdevice\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/AppData/Roaming/Python/Python39/site-packages/transformers/models/bert/modeling_bert.py?line=952'>953</a>\u001b[0m \u001b[39m# past_key_values_length\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "PATH = './saved_model'\n",
    "sample_input_ids = train_dataset[0]['input_ids']\n",
    "# print(sample_input_ids)\n",
    "sample_att_mask = train_dataset[0]['attention_mask']\n",
    "# print(sample_att_mask)\n",
    "\n",
    "torch.jit.save(torch.jit.trace(model,(sample_input_ids, sample_att_mask) ), os.path.join(PATH, \"model.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "PATH = './saved_model'\n",
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "loaded_model = torch.load(os.path.join(PATH, \"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'collections.OrderedDict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\BSCS 3- 2ND SEM\\9334-CS321-ARTIFICIAL_INTELLIGENCE\\GithubFinalProj\\hate-speech-model\\ai_model\\model_all\\bert_all_lightning.ipynb Cell 27'\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BSCS%203-%202ND%20SEM/9334-CS321-ARTIFICIAL_INTELLIGENCE/GithubFinalProj/hate-speech-model/ai_model/model_all/bert_all_lightning.ipynb#ch0000024?line=12'>13</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BSCS%203-%202ND%20SEM/9334-CS321-ARTIFICIAL_INTELLIGENCE/GithubFinalProj/hate-speech-model/ai_model/model_all/bert_all_lightning.ipynb#ch0000024?line=13'>14</a>\u001b[0m     test_input_ids, test_att_mask \u001b[39m=\u001b[39m test_text[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m], test_text[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/BSCS%203-%202ND%20SEM/9334-CS321-ARTIFICIAL_INTELLIGENCE/GithubFinalProj/hate-speech-model/ai_model/model_all/bert_all_lightning.ipynb#ch0000024?line=14'>15</a>\u001b[0m     _, output \u001b[39m=\u001b[39m loaded_model(test_input_ids, test_att_mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BSCS%203-%202ND%20SEM/9334-CS321-ARTIFICIAL_INTELLIGENCE/GithubFinalProj/hate-speech-model/ai_model/model_all/bert_all_lightning.ipynb#ch0000024?line=16'>17</a>\u001b[0m \u001b[39m# _, output = model(test_input_ids.unsqueeze(0), test_att_mask.unsqueeze(0),None)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BSCS%203-%202ND%20SEM/9334-CS321-ARTIFICIAL_INTELLIGENCE/GithubFinalProj/hate-speech-model/ai_model/model_all/bert_all_lightning.ipynb#ch0000024?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTASK 1\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'collections.OrderedDict' object is not callable"
     ]
    }
   ],
   "source": [
    "test_text = 'Hey asshole, Go kill yourself'\n",
    "test_text = tokenizer.encode_plus(\n",
    "    test_text,\n",
    "    add_special_tokens=True,\n",
    "    max_length=256,\n",
    "    return_token_type_ids=False,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_input_ids, test_att_mask = test_text['input_ids'], test_text['attention_mask']\n",
    "    _, output = loaded_model(test_input_ids, test_att_mask)\n",
    "\n",
    "# _, output = model(test_input_ids.unsqueeze(0), test_att_mask.unsqueeze(0),None)\n",
    "print('TASK 1')\n",
    "print(output[0])\n",
    "print(task1_id2label)\n",
    "print(task1_id2label[int(torch.argmax(output[0]))])\n",
    "\n",
    "print('TASK 2')\n",
    "print(output[1])\n",
    "print(task2_id2label)\n",
    "print(task2_id2label[int(torch.argmax(output[1]))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6ff18a2c3a5d785561d0cdab5ccf9507df71581ec286f02d4b39c00f7c48e831"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ai-ds-test4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
