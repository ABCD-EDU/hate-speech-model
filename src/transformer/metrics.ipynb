{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC:\n",
    "* [Accuracy Score](#first-bullet)\n",
    "* [Second Bullet Header](#second-bullet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from classes.TwitterNeuralNet import TwitterNeuralNet\n",
    "from classes.TwitterDataset import TwitterDataset\n",
    "\n",
    "from torchmetrics.functional import accuracy, f1_score, auroc\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "---------LOADING MODEL---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('./config/config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "print('---------LOADING MODEL---------')\n",
    "PATH = './torch_model'\n",
    "loaded_model = TwitterNeuralNet(bert_model_name=config['bert_model_name'])  # Task1->2 labels | Task2->3 Labels\n",
    "loaded_model.load_state_dict(torch.load(\n",
    "    os.path.join(PATH, config[\"trained_model_name\"]), map_location=device))\n",
    "\n",
    "loaded_model = loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "loaded_model.freeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../../res/preprocessed/test_final.csv')\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['bert_model_name'])\n",
    "test_dataset = TwitterDataset(\n",
    "    test_df, tokenizer, max_token_len=config['max_token_len'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LABEL_COLUMNS = list(test_df.columns)\n",
    "LABEL_COLUMNS.remove('text')\n",
    "\n",
    "TASK1_LABELS = LABEL_COLUMNS[:3]\n",
    "TASK2_LABELS = LABEL_COLUMNS[3:4]\n",
    "TASK3_LABELS = LABEL_COLUMNS[4:]\n",
    "\n",
    "\n",
    "task1_id2label = {idx: label for idx, label in enumerate(TASK1_LABELS)}\n",
    "task1_label2id = {label: idx for idx, label in enumerate(TASK1_LABELS)}\n",
    "\n",
    "task2_label2id = {label: idx for idx, label in enumerate(TASK2_LABELS)}\n",
    "task2_id2label = {idx: label for idx, label in enumerate(TASK2_LABELS)}\n",
    "\n",
    "task3_label2id = {label: idx for idx, label in enumerate(TASK3_LABELS)}\n",
    "task3_id2label = {idx: label for idx, label in enumerate(TASK3_LABELS)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task1_accuracy_score: 0.774350106716156\n",
      "task2_accuracy_score: 0.9362218379974365\n",
      "task2_accuracy_score: 0.8890814781188965\n"
     ]
    }
   ],
   "source": [
    "task1_predictions = []\n",
    "task1_labels = []\n",
    "\n",
    "task2_predictions = []\n",
    "task2_labels = []\n",
    "\n",
    "task3_predictions = []\n",
    "task3_labels = []\n",
    "\n",
    "predictions_labels_stacked_dict = {}\n",
    "\n",
    "def get_accuracy_score():\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, item in enumerate(test_dataset):\n",
    "            # if idx == 20:\n",
    "            #     break\n",
    "            _, prediction = loaded_model(\n",
    "                item['input_ids'].unsqueeze(dim=0).to(device),\n",
    "                item['attention_mask'].unsqueeze(dim=0).to(device)\n",
    "            )\n",
    "\n",
    "            task1_predictions.append(prediction[0].flatten())\n",
    "            task2_predictions.append(prediction[1].flatten())\n",
    "            task3_predictions.append(prediction[2].flatten())\n",
    "\n",
    "            task1_labels.append(item['labels1'].int())\n",
    "            task2_labels.append(item['labels2'].int())\n",
    "            task3_labels.append(item['labels3'].int())\n",
    "\n",
    "    task1_predictions_stacked = torch.stack(task1_predictions).detach().cpu()\n",
    "    task1_labels_stacked = torch.stack(task1_labels).detach().cpu()\n",
    "    predictions_labels_stacked_dict['task1_pred'] = task1_predictions_stacked\n",
    "    predictions_labels_stacked_dict['task1_label'] = task1_labels_stacked\n",
    "\n",
    "    task2_predictions_stacked = torch.stack(task2_predictions).detach().cpu()\n",
    "    task2_labels_stacked = torch.stack(task2_labels).detach().cpu()\n",
    "    predictions_labels_stacked_dict['task2_pred'] = task2_predictions_stacked\n",
    "    predictions_labels_stacked_dict['task2_label'] = task2_labels_stacked\n",
    "\n",
    "    task3_predictions_stacked = torch.stack(task3_predictions).detach().cpu()\n",
    "    task3_labels_stacked = torch.stack(task3_labels).detach().cpu()\n",
    "    predictions_labels_stacked_dict['task3_pred'] = task3_predictions_stacked\n",
    "    predictions_labels_stacked_dict['task3_label'] = task3_labels_stacked\n",
    "\n",
    "    THRESHOLD = 0.5\n",
    "\n",
    "    task1_accuracy_score = accuracy(\n",
    "        task1_predictions_stacked, task1_labels_stacked, threshold=THRESHOLD)\n",
    "\n",
    "    task2_accuracy_score = accuracy(\n",
    "        task2_predictions_stacked, task2_labels_stacked, threshold=THRESHOLD)\n",
    "\n",
    "    task3_accuracy_score = accuracy(\n",
    "        task3_predictions_stacked, task3_labels_stacked, threshold=THRESHOLD)\n",
    "\n",
    "    print(f'task1_accuracy_score: {task1_accuracy_score}')\n",
    "    print(f'task2_accuracy_score: {task2_accuracy_score}')\n",
    "    print(f'task2_accuracy_score: {task3_accuracy_score}')\n",
    "    \n",
    "get_accuracy_score()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC per tag\n",
      "HATE: 0.8965604305267334\n",
      "NOT: 0.824948787689209\n",
      "OFFN: 0.7422533631324768\n"
     ]
    }
   ],
   "source": [
    "print(\"AUROC per tag\")\n",
    "for i, name in enumerate(TASK1_LABELS):\n",
    "  task1_auroc = auroc(predictions_labels_stacked_dict['task1_pred'][:,i].to(device), \n",
    "                      predictions_labels_stacked_dict['task1_label'][:, i].to(\n",
    "      device), num_classes=len(TASK1_LABELS), pos_label=1)\n",
    "  print(f\"{name}: {task1_auroc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC per tag\n",
      "Race: 0.937118649482727\n",
      "Religion: 0.9716429710388184\n",
      "Gender: 0.9373893737792969\n",
      "Other: 0.8594638109207153\n",
      "None: 0.8242061734199524\n"
     ]
    }
   ],
   "source": [
    "print(\"AUROC per tag\")\n",
    "for i, name in enumerate(TASK3_LABELS):\n",
    "  task3_auroc = auroc(predictions_labels_stacked_dict['task3_pred'][:, i].to(device),\n",
    "                      predictions_labels_stacked_dict['task3_label'][:, i].to(\n",
    "      device), num_classes=len(TASK3_LABELS), pos_label=1)\n",
    "  print(f\"{name}: {task3_auroc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous-multioutput and multilabel-indicator targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\BSCS 3- 2ND SEM\\9334-CS321-ARTIFICIAL_INTELLIGENCE\\GithubFinalProj\\hate-speech-model\\src\\transformer\\metrics.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/BSCS%203-%202ND%20SEM/9334-CS321-ARTIFICIAL_INTELLIGENCE/GithubFinalProj/hate-speech-model/src/transformer/metrics.ipynb#ch0000007?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(classification_report(\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/BSCS%203-%202ND%20SEM/9334-CS321-ARTIFICIAL_INTELLIGENCE/GithubFinalProj/hate-speech-model/src/transformer/metrics.ipynb#ch0000007?line=1'>2</a>\u001b[0m     predictions_labels_stacked_dict[\u001b[39m'\u001b[39;49m\u001b[39mtask3_pred\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/BSCS%203-%202ND%20SEM/9334-CS321-ARTIFICIAL_INTELLIGENCE/GithubFinalProj/hate-speech-model/src/transformer/metrics.ipynb#ch0000007?line=2'>3</a>\u001b[0m     predictions_labels_stacked_dict[\u001b[39m'\u001b[39;49m\u001b[39mtask3_label\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/BSCS%203-%202ND%20SEM/9334-CS321-ARTIFICIAL_INTELLIGENCE/GithubFinalProj/hate-speech-model/src/transformer/metrics.ipynb#ch0000007?line=3'>4</a>\u001b[0m     target_names\u001b[39m=\u001b[39;49mTASK3_LABELS,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/BSCS%203-%202ND%20SEM/9334-CS321-ARTIFICIAL_INTELLIGENCE/GithubFinalProj/hate-speech-model/src/transformer/metrics.ipynb#ch0000007?line=4'>5</a>\u001b[0m     zero_division\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/BSCS%203-%202ND%20SEM/9334-CS321-ARTIFICIAL_INTELLIGENCE/GithubFinalProj/hate-speech-model/src/transformer/metrics.ipynb#ch0000007?line=5'>6</a>\u001b[0m ))\n",
      "File \u001b[1;32m~\\.conda\\envs\\ai-ds-test4\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2110\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/sklearn/metrics/_classification.py?line=1997'>1998</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclassification_report\u001b[39m(\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/sklearn/metrics/_classification.py?line=1998'>1999</a>\u001b[0m     y_true,\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/sklearn/metrics/_classification.py?line=1999'>2000</a>\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/sklearn/metrics/_classification.py?line=2006'>2007</a>\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/sklearn/metrics/_classification.py?line=2007'>2008</a>\u001b[0m ):\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/sklearn/metrics/_classification.py?line=2008'>2009</a>\u001b[0m     \u001b[39m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/sklearn/metrics/_classification.py?line=2009'>2010</a>\u001b[0m \n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/sklearn/metrics/_classification.py?line=2010'>2011</a>\u001b[0m \u001b[39m    Read more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/sklearn/metrics/_classification.py?line=2106'>2107</a>\u001b[0m \u001b[39m    <BLANKLINE>\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/sklearn/metrics/_classification.py?line=2107'>2108</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/sklearn/metrics/_classification.py?line=2109'>2110</a>\u001b[0m     y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/sklearn/metrics/_classification.py?line=2111'>2112</a>\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/sklearn/metrics/_classification.py?line=2112'>2113</a>\u001b[0m         labels \u001b[39m=\u001b[39m unique_labels(y_true, y_pred)\n",
      "File \u001b[1;32m~\\.conda\\envs\\ai-ds-test4\\lib\\site-packages\\sklearn\\metrics\\_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/sklearn/metrics/_classification.py?line=89'>90</a>\u001b[0m     y_type \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m     <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/sklearn/metrics/_classification.py?line=91'>92</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(y_type) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/sklearn/metrics/_classification.py?line=92'>93</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/sklearn/metrics/_classification.py?line=93'>94</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mClassification metrics can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt handle a mix of \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m targets\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/sklearn/metrics/_classification.py?line=94'>95</a>\u001b[0m             type_true, type_pred\n\u001b[0;32m     <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/sklearn/metrics/_classification.py?line=95'>96</a>\u001b[0m         )\n\u001b[0;32m     <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/sklearn/metrics/_classification.py?line=96'>97</a>\u001b[0m     )\n\u001b[0;32m     <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/sklearn/metrics/_classification.py?line=98'>99</a>\u001b[0m \u001b[39m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Arian/.conda/envs/ai-ds-test4/lib/site-packages/sklearn/metrics/_classification.py?line=99'>100</a>\u001b[0m y_type \u001b[39m=\u001b[39m y_type\u001b[39m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous-multioutput and multilabel-indicator targets"
     ]
    }
   ],
   "source": [
    "print(classification_report(\n",
    "    predictions_labels_stacked_dict['task3_pred'],\n",
    "    predictions_labels_stacked_dict['task3_label'],\n",
    "    target_names=TASK3_LABELS,\n",
    "    zero_division=0\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6ff18a2c3a5d785561d0cdab5ccf9507df71581ec286f02d4b39c00f7c48e831"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ai-ds-test4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
